{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Whisper Model fine-tuning for ASR**\n",
    "\n",
    "In this file, you can easily fine-tune different variations of the Whisper model to your specific multilingual data based on a simple manifest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIAyaSgvebzT",
    "outputId": "c8454785-01fb-46fb-a13e-0cec47d9186e"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install --upgrade librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ErTzMUFPdXi4"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from datasets import Audio\n",
    "import gc\n",
    "import evaluate\n",
    "import torch\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run the code on googlecolab you can add the googledrive. If you want to run locally, you dont need to this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6YIeWshdohD",
    "outputId": "b0fb331c-7942-499b-eb00-a5422c656383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data prepration**\n",
    "In this section, you can create the train.csv and test.csv files based on Kaldi's file. If you do not want to use Kaldi's files, you can easily write your code to generate train.csv and test.csv in this format:<br>\n",
    "(one audio file and its corresponding text in each row)\n",
    "\n",
    "     Path \t                                                         Sentence\n",
    "______________________________________________________________________\n",
    "/home/rf/kaldi/egs/Aref_mini_librispeech/s5/corpus/010/s010u128w.wav&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;lots of foreign movies have subtitles<br>\n",
    "/home/rf/kaldi/egs/Aref_mini_librispeech/s5/corpus/029/s009u423n.wav&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;smash lightbulbs and their cash value will diminish to nothing<br>\n",
    "/home/rf/kaldi/egs/Aref_mini_librispeech/s5/corpus/011/s011u328w.wav&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gregory and tom chose to watch cartoons in the afternoon<br>\n",
    "/home/rf/kaldi/egs/Aref_mini_librispeech/s5/corpus/018/s018u445w.wav&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;what is this large thing by the ironing board<br>\n",
    "/home/rf/kaldi/egs/Aref_mini_librispeech/s5/corpus/017/s017u372w.wav&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that diagram makes sense only after much study<br>\n",
    "/home/rf/kaldi/egs/Aref_mini_librispeech/s5/corpus/023/s003u207n.wav&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do you hear the sleigh bells ringing<br> \n",
    ".............................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6YIeWshdohD",
    "outputId": "b0fb331c-7942-499b-eb00-a5422c656383"
   },
   "outputs": [],
   "source": [
    "## Preparing train.csv and test.csv based on the Kaldi's files: \"text\" and \"wav.scp\"\n",
    "\n",
    "# Read the first .txt file and store the second elements\n",
    "file1_values = []\n",
    "with open('wav_train.scp', 'r') as file1:\n",
    "    for line in file1:\n",
    "        values = line.strip().split() \n",
    "        if len(values) >= 2: \n",
    "            file1_values.append(values[1])  # piking up the address of each audio file\n",
    "\n",
    "# Read the second .txt file and store all elements except the first\n",
    "file2_values = []\n",
    "with open('text_train', 'r') as file2:\n",
    "    for line in file2:\n",
    "        values = line.strip().split()  \n",
    "        if len(values) > 1:  \n",
    "            file2_values.append(' '.join(values[1:]))  # Join all elements except the first that was audio filename\n",
    "\n",
    "# Write the values into a new CSV file\n",
    "with open('train.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Path', 'Sentence'])  # Write the header row with updated column names\n",
    "\n",
    "    for value1, value2 in zip(file1_values, file2_values):\n",
    "        writer.writerow([value1, value2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing test.csv and test.csv based on the Kaldi's files: \"text\" and \"wav.scp\"\n",
    "\n",
    "# Read the first .txt file and store the second elements\n",
    "file1_values = []\n",
    "with open('wav_test.scp', 'r') as file1:\n",
    "    for line in file1:\n",
    "        values = line.strip().split()  # Assuming space-separated elements\n",
    "        if len(values) >= 2:  # Check if there is a second element\n",
    "            file1_values.append(values[1])  # piking up the address of each audio file\n",
    "\n",
    "# Read the second .txt file and store all elements except the first\n",
    "file2_values = []\n",
    "with open('text_test', 'r') as file2:\n",
    "    for line in file2:\n",
    "        values = line.strip().split()  # Assuming space-separated elements\n",
    "        if len(values) > 1:  # Check if there are more than one elements\n",
    "            file2_values.append(' '.join(values[1:]))  # Join all elements except the first that was audio filename\n",
    "\n",
    "# Write the values into a new CSV file\n",
    "with open('test.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Path', 'Sentence'])  # Write the header row with updated column names\n",
    "\n",
    "    # Write the corresponding values from both files\n",
    "    for value1, value2 in zip(file1_values, file2_values):\n",
    "        writer.writerow([value1, value2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Training**\n",
    "You can easily train the Whisper model using the train.csv and test.csv files.\n",
    "If you don't have enough GPU memory, you can use lighter versions of the Whisper model like \"tiny\" and \"base\" instead of the \"small\" version.\n",
    "Please change the files' location to yours. In addition, you can switch the \"language\" parameter to your dataset's language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BFCL6htVdgqU"
   },
   "outputs": [],
   "source": [
    "## we will load the both of the data here.\n",
    "train_df = pd.read_csv(\"/home/rf/makingdata_for_wav2vec/train.csv\")\n",
    "test_df = pd.read_csv(\"/home/rf/makingdata_for_wav2vec/test.csv\")\n",
    "\n",
    "## convert the pandas dataframes to dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "## convert the sample rate of every audio files using cast_column function\n",
    "train_dataset = train_dataset.cast_column(\"Path\", Audio(sampling_rate=16000))\n",
    "test_dataset = test_dataset.cast_column(\"Path\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97iZwbOBfCYG",
    "outputId": "30568be8-1355-45ff-fda5-620e46eb807a"
   },
   "outputs": [],
   "source": [
    "## Import feature extractor\n",
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\") # openai/whisper-base\n",
    "\n",
    "## Load WhisperTokenizer\n",
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")\n",
    "\n",
    "## Combine To Create A WhisperProcessor\n",
    "from transformers import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"English\", task=\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bkzy5dl5fua0"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    # compute log-Mel input features from input audio array\n",
    "    audio = examples[\"Path\"]\n",
    "    examples[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "    del examples[\"Path\"]\n",
    "    sentences = examples[\"Sentence\"]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    examples[\"labels\"] = tokenizer(sentences).input_ids\n",
    "    del examples[\"Sentence\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "a6c714a303e041d9a988bac9668d4b43",
      "9fc91c5242cf448b94eb5affe65a4436",
      "fae1df0f195d49db88a0e3c0c8d77515",
      "3e4e7512361840afa0a1c9b1694368f6",
      "25461117b2bb4d6691e42f22ee67351a",
      "b0f2e27fc5a14c92b4227f84af6ab25c",
      "83d44699699b47c5b7f45cd2df216877",
      "648f13d928ab41529b2c0c79137d6fce",
      "8ba8e9e23b4846958dc133f697305cf3",
      "5113aef4f87145a18d0f33f2677993da",
      "a0d0ccfec110418c931d109db75b02fd"
     ]
    },
    "id": "8KAN2C7afvSt",
    "outputId": "ff603fc8-a5ab-4480-c258-91da2a9221dc"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(prepare_dataset, num_proc=1)\n",
    "test_dataset = test_dataset.map(prepare_dataset, num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VqnjCWKhhV8N"
   },
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "## lets initiate the data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HVDht-ZQhqE-"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFAxtjQThqmV"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9AqFiqnnhxqV"
   },
   "outputs": [],
   "source": [
    "# Load a Pre-Trained Checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "nYGGjlxIh3ZB"
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your dataset and Whisper model size, you can change the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eaTRy_J4h_vN"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=15000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step1 - training...\")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step2 - \")\n",
    "\n",
    "metrics = train_result.metrics\n",
    "print(\"step3\")\n",
    "max_train_samples = len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "print(\"step4 - saving the model...\")\n",
    "trainer.save_model()\n",
    "print(\"model created!\")\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "25461117b2bb4d6691e42f22ee67351a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e4e7512361840afa0a1c9b1694368f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5113aef4f87145a18d0f33f2677993da",
      "placeholder": "​",
      "style": "IPY_MODEL_a0d0ccfec110418c931d109db75b02fd",
      "value": " 1945/13112 [01:33&lt;05:14, 35.53 examples/s]"
     }
    },
    "5113aef4f87145a18d0f33f2677993da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "648f13d928ab41529b2c0c79137d6fce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83d44699699b47c5b7f45cd2df216877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ba8e9e23b4846958dc133f697305cf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9fc91c5242cf448b94eb5affe65a4436": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0f2e27fc5a14c92b4227f84af6ab25c",
      "placeholder": "​",
      "style": "IPY_MODEL_83d44699699b47c5b7f45cd2df216877",
      "value": "Map (num_proc=8):  15%"
     }
    },
    "a0d0ccfec110418c931d109db75b02fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6c714a303e041d9a988bac9668d4b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9fc91c5242cf448b94eb5affe65a4436",
       "IPY_MODEL_fae1df0f195d49db88a0e3c0c8d77515",
       "IPY_MODEL_3e4e7512361840afa0a1c9b1694368f6"
      ],
      "layout": "IPY_MODEL_25461117b2bb4d6691e42f22ee67351a"
     }
    },
    "b0f2e27fc5a14c92b4227f84af6ab25c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fae1df0f195d49db88a0e3c0c8d77515": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_648f13d928ab41529b2c0c79137d6fce",
      "max": 13112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ba8e9e23b4846958dc133f697305cf3",
      "value": 1945
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
